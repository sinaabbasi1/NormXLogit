{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e0310a",
   "metadata": {},
   "source": [
    "**Note:** The implementation and preprocessing codes were sourced from https://github.com/hmohebbi/ValueZeroing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a74fde2",
   "metadata": {},
   "source": [
    "# The Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "915bd5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use the GPU: Quadro RTX 8000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(f\"cuda:{0}\")\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4aa92e",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b22eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NUM_LABELS = {\n",
    "    \"ana\": 2,\n",
    "    \"dna\": 2,\n",
    "    \"dnaa\": 2,\n",
    "    \"rpsv\": 2,\n",
    "    \"darn\": 2,\n",
    "    \"NA\": 2,\n",
    "}\n",
    "\n",
    "blimp_to_label = {\n",
    "    'singular': 0,\n",
    "    'plural': 1,\n",
    "}\n",
    "\n",
    "MODEL_PATH = {\n",
    "    'bert': 'bert-base-uncased',\n",
    "    'roberta': 'roberta-base',\n",
    "    'electra': 'google/electra-base-generator',\n",
    "    'deberta': 'microsoft/deberta-v3-base'\n",
    "}\n",
    "\n",
    "BLIMP_TASKS = [\n",
    "    \"ana\",\n",
    "    'dna',\n",
    "    \"dnaa\",\n",
    "    \"rpsv\",\n",
    "    \"darn\",\n",
    "    \"NA\",\n",
    "]\n",
    "\n",
    "def blimp_to_features(data, tokenizer, max_length, input_masking, mlm):\n",
    "    all_features = []\n",
    "    for example in data:\n",
    "        text = example['sentence_good']\n",
    "        tokens = []\n",
    "        cue_indices = []\n",
    "        # token to id\n",
    "        for w_ind, word in enumerate(text):\n",
    "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "            if w_ind in example['cue_indices']:\n",
    "                cue_indices.append(len(tokens))\n",
    "            if w_ind == example['target_index']:\n",
    "                target_index = len(tokens)\n",
    "            tokens.extend(ids)\n",
    "        \n",
    "        tokens = [tokenizer.cls_token_id] + tokens + [tokenizer.sep_token_id]\n",
    "        cue_indices = [x+1 for x in cue_indices] # 'cause of adding cls\n",
    "        target_index += 1 # 'cause of adding cls\n",
    "        if input_masking:\n",
    "            tokens[target_index] = tokenizer.mask_token_id\n",
    "\n",
    "        # padding\n",
    "        length = len(tokens)\n",
    "        inputs = {}\n",
    "        inputs['input_ids'] = tokens if max_length is None else tokens + [tokenizer.pad_token_id]*(max_length - length)\n",
    "        inputs['attention_mask'] = [1]*length if max_length is None else [1]*length + [0]*(max_length - length)\n",
    "        inputs['token_type_ids'] = [0]*length if max_length is None else [0]*max_length\n",
    "        inputs['target_index'] = target_index\n",
    "        inputs['labels'] = tokenizer.convert_tokens_to_ids(example['good_word']) if mlm else blimp_to_label[example['labels']]\n",
    "        inputs['good_token_id'] = tokenizer.convert_tokens_to_ids(example['good_word'])\n",
    "        inputs['bad_token_id'] = tokenizer.convert_tokens_to_ids(example['bad_word'])\n",
    "\n",
    "        # As a 2d tensor, we need all rows to have the same length. So, we add -1 to the end of each list.\n",
    "        inputs['cue_indices'] = cue_indices + (10 - len(cue_indices)) * [-1]\n",
    "\n",
    "        all_features.append(inputs)\n",
    "    return all_features[0] if len(all_features) == 1 else all_features\n",
    "\n",
    "PREPROCESS_FUNC = {\n",
    "    'ana': blimp_to_features,\n",
    "    'dna': blimp_to_features,\n",
    "    'dnaa': blimp_to_features,\n",
    "    'rpsv': blimp_to_features,\n",
    "    'darn': blimp_to_features,\n",
    "    'NA': blimp_to_features,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110a3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_GPU = 0\n",
    "MODEL_NAME = 'roberta'\n",
    "FIXED = False\n",
    "CHECKPOINT = \"full\"\n",
    "METRIC = 'cosine' \n",
    "TASK = \"NA\"\n",
    "SPLIT = \"test\"\n",
    "MAX_LENGTH = 32\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "PER_DEVICE_BATCH_SIZE = 1\n",
    "SAVE_SCORES = False\n",
    "\n",
    "INPUT_MASKING = True\n",
    "MLM = True\n",
    "LEARNING_RATE = 3e-5\n",
    "LR_SCHEDULER_TYPE = \"linear\" \n",
    "WARMUP_RATIO = 0.1\n",
    "SEED = 42\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    load_from_disk,\n",
    "    load_metric,\n",
    ")\n",
    "\n",
    "from modeling.customized_modeling_bert import BertForMaskedLM\n",
    "from modeling.customized_modeling_roberta import RobertaForMaskedLM\n",
    "# from modeling.customized_modeling_electra import ElectraForMaskedLM\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "set_seed(SEED)\n",
    "\n",
    "# Load Dataset\n",
    "if TASK in BLIMP_TASKS:\n",
    "    data_path = f\"./BLIMP Dataset/{MODEL_NAME}/\"\n",
    "    eval_data = load_from_disk(data_path)[SPLIT]\n",
    "else:\n",
    "    print(\"Not implemented yet!\")\n",
    "    exit()\n",
    "\n",
    "num_labels = NUM_LABELS[TASK]\n",
    "\n",
    "# Load Tokenizer & Model\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH[MODEL_NAME], num_labels=num_labels)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH[MODEL_NAME])  \n",
    "\n",
    "if MODEL_NAME == \"bert\":\n",
    "    model = BertForMaskedLM.from_pretrained(MODEL_PATH[MODEL_NAME], config=config)\n",
    "elif MODEL_NAME == \"roberta\":\n",
    "    model = RobertaForMaskedLM.from_pretrained(MODEL_PATH[MODEL_NAME], config=config)\n",
    "# elif MODEL_NAME == \"electra\":\n",
    "#     model = ElectraForMaskedLM.from_pretrained(MODEL_PATH[MODEL_NAME], config=config)\n",
    "else:\n",
    "    print(\"model doesn't exist\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Preprocessing\n",
    "eval_dataset = PREPROCESS_FUNC[TASK](eval_data, tokenizer, max_length=None, input_masking=INPUT_MASKING, mlm=MLM)\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=PER_DEVICE_BATCH_SIZE)\n",
    "num_examples = len(eval_dataset)\n",
    "\n",
    "# metric & Loss\n",
    "metric = load_metric(\"accuracy\")\n",
    "loss_fct = CrossEntropyLoss()\n",
    "\n",
    "tag = \"forseqclassification_\"\n",
    "tag += \"pretrained\" if FIXED else \"finetuned\" \n",
    "if MLM:\n",
    "    tag += \"_MLM\"\n",
    "\n",
    "masking_tag = \"masked\" if INPUT_MASKING else \"full\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97e71cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "DISTANCE_FUNC = {'cosine': cosine_distances}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b596113b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'{MODEL_NAME}_full_{tag}_epoch{NUM_TRAIN_EPOCHS}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a33f9",
   "metadata": {},
   "source": [
    "# Value Zeroing Importance Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rollout\n",
    "def compute_joint_attention(att_mat, res=True):\n",
    "    if res:\n",
    "        residual_att = np.eye(att_mat.shape[1])[None,...]\n",
    "        att_mat = att_mat + residual_att\n",
    "        att_mat = att_mat / att_mat.sum(axis=-1)[...,None]\n",
    "    \n",
    "    joint_attentions = np.zeros(att_mat.shape)\n",
    "    layers = joint_attentions.shape[0]\n",
    "    joint_attentions[0] = att_mat[0]\n",
    "    for i in np.arange(1,layers):\n",
    "        joint_attentions[i] = att_mat[i].dot(joint_attentions[i-1])\n",
    "        \n",
    "    return joint_attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f9eea",
   "metadata": {},
   "source": [
    "Computing Value Zeroing scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d72a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_valuezeroing_scores = [] # (#layers, #seq_length, #seq_length)\n",
    "all_rollout_valuezeroing_scores = [] # (#layers, #seq_length, #seq_length)\n",
    "\n",
    "progress_bar = tqdm(range(num_examples))\n",
    "for step, inputs in enumerate(eval_dataloader):\n",
    "    \n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs['input_ids'],\n",
    "                        attention_mask=inputs['attention_mask'],\n",
    "                        # token_type_ids=inputs['token_type_ids'], \n",
    "                        output_hidden_states=True, output_attentions=False)\n",
    "\n",
    "    org_hidden_states = torch.stack(outputs['hidden_states']).squeeze(1)\n",
    "    input_shape = inputs['input_ids'].size() \n",
    "    batch_size, seq_length = input_shape\n",
    "\n",
    "    ## layerwise zeroing value\n",
    "    score_matrix = np.zeros((config.num_hidden_layers, seq_length, seq_length))\n",
    "    for l, layer_module in enumerate(model.roberta.encoder.layer): # change based on the model\n",
    "        for t in range(seq_length):\n",
    "            \n",
    "            if MODEL_NAME == 'bert':\n",
    "                extended_blanking_attention_mask: torch.Tensor = model.bert.get_extended_attention_mask(inputs['attention_mask'], input_shape).to(device)            \n",
    "            elif MODEL_NAME == 'roberta':\n",
    "                extended_blanking_attention_mask: torch.Tensor = model.roberta.get_extended_attention_mask(inputs['attention_mask'], input_shape).to(device)            \n",
    "                \n",
    "            with torch.no_grad():\n",
    "                layer_outputs = layer_module(org_hidden_states[l].unsqueeze(0), # previous layer's original output \n",
    "                                            attention_mask=extended_blanking_attention_mask,\n",
    "                                            output_attentions=False,\n",
    "                                            zero_value_index=t,\n",
    "                                            )\n",
    "                \n",
    "            hidden_states = layer_outputs[0].squeeze().detach().cpu().numpy()\n",
    "            \n",
    "            # compute similarity between original and new outputs\n",
    "            # cosine\n",
    "            x = hidden_states\n",
    "            y = org_hidden_states[l+1].detach().cpu().numpy()\n",
    "\n",
    "            distances = DISTANCE_FUNC[METRIC](x, y).diagonal()\n",
    "            score_matrix[l, :, t] = distances\n",
    "\n",
    "    score_matrix = score_matrix / np.sum(score_matrix, axis=-1, keepdims=True)\n",
    "    \n",
    "    all_valuezeroing_scores.append(score_matrix)\n",
    "    all_rollout_valuezeroing_scores.append(compute_joint_attention(score_matrix, res=False))\n",
    "    \n",
    "    progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2134c07a",
   "metadata": {},
   "source": [
    "# Alignment Metrics\n",
    "\n",
    "Here, we compute Dot Product and Average Precision. At first, let's define a method to compute Average Precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the tensors from -1 padding\n",
    "def CI_cleaner(CI):\n",
    "    first_pad_index = torch.where(CI == -1)[0][0].item() # We have used -1 as paddings of CIs\n",
    "    return CI[:first_pad_index]\n",
    "\n",
    "# Calculating Precision\n",
    "def precision(TP, FP):\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "# Calculating Recall\n",
    "def recall(TP, FN):\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "# Calculating Average Precision\n",
    "def avg_precision(topk, CI):\n",
    "    R_base = 0 # The starting recall before the first round\n",
    "    AP, TP, FP, FN = 0, 0, 0, len(CI)\n",
    "    previous_recall = R_base\n",
    "    for i in range(len(topk)):\n",
    "        if topk[i] in CI:\n",
    "            TP += 1\n",
    "            FN -= 1\n",
    "        else:\n",
    "            FP += 1\n",
    "\n",
    "        AP += (recall(TP, FN) -  previous_recall) * precision(TP, FP)\n",
    "        previous_recall = recall(TP, FN)\n",
    "\n",
    "    return AP\n",
    "\n",
    "topk = torch.tensor([1, 0, 3, 4, 2])\n",
    "CI = torch.tensor([3, -1, -1, -1])\n",
    "# CI = CI_cleaner(CI)\n",
    "# print(avg_precision(topk, CI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5ced9f19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a7283808fe43059059de8b70407ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Dot Product ###\n",
      "tensor([[0.1210],\n",
      "        [0.1553],\n",
      "        [0.1273],\n",
      "        [0.0903],\n",
      "        [0.1152],\n",
      "        [0.1271],\n",
      "        [0.1091],\n",
      "        [0.1366],\n",
      "        [0.1186],\n",
      "        [0.1195],\n",
      "        [0.1318],\n",
      "        [0.1573]], dtype=torch.float64)\n",
      "### Average Precision ###\n",
      "[0.33297120160731064, 0.35506991249036896, 0.3675264035153822, 0.2629552616098428, 0.31504201560673706, 0.35902219496619014, 0.3075139568833522, 0.3653852097191506, 0.3403612887286204, 0.307320248072889, 0.33685834170319817, 0.3594334375077499]\n"
     ]
    }
   ],
   "source": [
    "diagram_layers = range(12)\n",
    "APs_vz = dict()\n",
    "\n",
    "for layer in diagram_layers:\n",
    "    APs_vz[f'layer{layer}'] = list()\n",
    "\n",
    "sum_vz_scores = 0\n",
    "\n",
    "model.eval()\n",
    "for i, batch_sample in enumerate(tqdm(eval_dataloader)):\n",
    "    \n",
    "    CI = CI_cleaner(batch_sample['cue_indices'][0]) # [0]: because we only have one sample in each batch\n",
    "    \n",
    "    ### Average Precision\n",
    "    batch_lengths = batch_sample['attention_mask'].sum(axis=-1)\n",
    "    mask_index = batch_sample['target_index'] # mask_index = target_index\n",
    "    # The contribution of each token in the sequence in building the rep. of target token for different layers\n",
    "    vz_importance = all_valuezeroing_scores[i][:, batch_sample['target_index']] # shape: [12, seq_len]\n",
    "    # Convert to torch tensor form numpy ndarray\n",
    "    vz_importance = torch.from_numpy(vz_importance)\n",
    "    # batch_lengths[0]: because we only have one sample in each batch\n",
    "    vz_importance_topk = torch.topk(vz_importance, k=batch_lengths[0], largest=True, dim=1).indices\n",
    "    \n",
    "    ### excluding mask_index\n",
    "    mask_index_tensor = torch.full_like(vz_importance_topk, mask_index.item())\n",
    "    # Create a mask that is True for elements not equal to mask_index\n",
    "    mask = vz_importance_topk != mask_index_tensor\n",
    "    # Apply the mask to exclude mask_index\n",
    "    vz_importance_topk_filtered = vz_importance_topk[mask].view(vz_importance_topk.size(0), -1)\n",
    "\n",
    "    for layer, layer_importance in enumerate(vz_importance_topk_filtered):\n",
    "        APs_vz[f'layer{layer}'].append(avg_precision(layer_importance, CI))\n",
    "        \n",
    "    ### Dot Product\n",
    "    # Remove mask_index and then normalize the scores.\n",
    "    vz_scores = all_valuezeroing_scores[i][:, batch_sample['target_index']]\n",
    "    vz_scores = torch.from_numpy(vz_scores)\n",
    "    vz_scores = torch.concat((vz_scores[:, :mask_index.item()], vz_scores[:, mask_index.item() + 1:]), dim=1)\n",
    "    vz_scores = vz_scores / vz_scores.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    if CI[-1].item() > mask_index.item():\n",
    "        CI_scores = vz_scores[:, CI - 1] # Because of the removed mask_index\n",
    "    else:\n",
    "        CI_scores = vz_scores[:, CI]\n",
    "    \n",
    "    # In case there are more than one cue indices (i.e. evidence)\n",
    "    if CI.shape[0] > 1:\n",
    "        CI_scores = CI_scores.sum(axis=1, keepdim=True)\n",
    "        \n",
    "    sum_vz_scores += CI_scores\n",
    "\n",
    "print(\"### Dot Product ###\")\n",
    "print(sum_vz_scores / len(eval_dataloader))\n",
    "\n",
    "print(\"### Average Precision ###\")\n",
    "temp_list = list()\n",
    "for layer in diagram_layers:\n",
    "    temp_list.append(sum(APs_vz[f'layer{layer}']) / len(APs_vz[f'layer{layer}']))\n",
    "\n",
    "print(temp_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b0e61b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1210, 0.1553, 0.1273, 0.0903, 0.1152, 0.1271, 0.1091, 0.1366, 0.1186,\n",
       "        0.1195, 0.1318, 0.1573])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[0.1210],\n",
    "        [0.1553],\n",
    "        [0.1273],\n",
    "        [0.0903],\n",
    "        [0.1152],\n",
    "        [0.1271],\n",
    "        [0.1091],\n",
    "        [0.1366],\n",
    "        [0.1186],\n",
    "        [0.1195],\n",
    "        [0.1318],\n",
    "        [0.1573]]).squeeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sina_pt_gpu",
   "language": "python",
   "name": "sina_pt_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
