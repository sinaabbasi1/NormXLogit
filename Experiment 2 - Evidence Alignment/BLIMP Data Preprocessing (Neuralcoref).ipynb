{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40b0bd0",
   "metadata": {},
   "source": [
    "**NOTE on Installation of `neuralcoref`:**\n",
    "\n",
    "First I installed the neuralcoref using the command: `conda install conda-forge::neuralcoref` </br> This will install neuralcoref V4.\n",
    "\n",
    "Then I ran `import neuralcoref` which downloaded a 41MB file to fix the `spacy.strings.StringStore size changed` problem.\n",
    "\n",
    "At last, I encountered with the `kernel died` problem which I fixed using `spacy==2.1.0` which was not provided using `conda` so I installed it using `pip`.\n",
    "\n",
    "Tadaaaa! It's show time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a74fde2",
   "metadata": {},
   "source": [
    "# Check the Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c5855c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3a8065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import neuralcoref\n",
    "neuralcoref.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fec82a",
   "metadata": {},
   "source": [
    "An example of how `neuralcoref` works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d15aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!python -m spacy download en\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e191416a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[My sister: [My sister, She], a dog: [a dog, him]]\n",
      "Angela: [Angela, She]\n",
      "Boston: [Boston, that city]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "doc1 = nlp('My sister has a dog. She loves him.')\n",
    "print(doc1._.coref_clusters)\n",
    "\n",
    "doc2 = nlp('Angela lives in Boston. She is quite happy in that city.')\n",
    "for ent in doc2.ents:\n",
    "    print(ent._.coref_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d75b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Angela: [Angela, She]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2._.coref_clusters[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe3c42",
   "metadata": {},
   "source": [
    "# BLIMP Data Preprocessing\n",
    "The code is provided by ValueZeroing repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5248bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "# import spacy\n",
    "# import neuralcoref\n",
    "from transformers import AutoTokenizer\n",
    "import sys, os\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(sys.modules[__name__].__file__), \"../..\")))\n",
    "# from utils.utils import MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4081598",
   "metadata": {},
   "source": [
    "Expanding the contractions of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f3e2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contraction_process(example):\n",
    "    example['sentence_good'] = example['sentence_good'].replace(\"wouldn't\", \"would not\")\n",
    "    example['sentence_bad'] = example['sentence_bad'].replace(\"wouldn't\", \"would not\")\n",
    "\n",
    "    example['sentence_good'] = example['sentence_good'].replace(\"couldn't\", \"could not\")\n",
    "    example['sentence_bad'] = example['sentence_bad'].replace(\"couldn't\", \"could not\")\n",
    "\n",
    "    example['sentence_good'] = example['sentence_good'].replace(\"shouldn't\", \"should not\")\n",
    "    example['sentence_bad'] = example['sentence_bad'].replace(\"shouldn't\", \"should not\")\n",
    "\n",
    "    example['sentence_good'] = example['sentence_good'].replace(\"won't\", \"will not\")\n",
    "    example['sentence_bad'] = example['sentence_bad'].replace(\"won't\", \"will not\")\n",
    "\n",
    "    example['sentence_good'] = example['sentence_good'].replace(\"can't\", \"cannot\")\n",
    "    example['sentence_bad'] = example['sentence_bad'].replace(\"can't\", \"cannot\")\n",
    "\n",
    "    example['sentence_good'] = example['sentence_good'].replace(\"don't\", \"do not\")\n",
    "    example['sentence_bad'] = example['sentence_bad'].replace(\"don't\", \"do not\")\n",
    "\n",
    "    example['sentence_good'] = example['sentence_good'].replace(\"doesn't\", \"does not\")\n",
    "    example['sentence_bad'] = example['sentence_bad'].replace(\"doesn't\", \"does not\")\n",
    "\n",
    "    example['sentence_good'] = example['sentence_good'].replace(\"didn't\", \"did not\")\n",
    "    example['sentence_bad'] = example['sentence_bad'].replace(\"didn't\", \"did not\")\n",
    "\n",
    "    example['sentence_good'] = example['sentence_good'].replace(\"isn't\", \"is not\")\n",
    "    example['sentence_bad'] = example['sentence_bad'].replace(\"isn't\", \"is not\")\n",
    "\n",
    "    example['sentence_good'] = example['sentence_good'].replace(\"aren't\", \"are not\")\n",
    "    example['sentence_bad'] = example['sentence_bad'].replace(\"aren't\", \"are not\")\n",
    "\n",
    "    example['sentence_good'] = example['sentence_good'].replace(\"wasn't\", \"was not\")\n",
    "    example['sentence_bad'] = example['sentence_bad'].replace(\"wasn't\", \"was not\")\n",
    "\n",
    "    example['sentence_good'] = example['sentence_good'].replace(\"weren't\", \"were not\")\n",
    "    example['sentence_bad'] = example['sentence_bad'].replace(\"weren't\", \"were not\")\n",
    "\n",
    "    example['sentence_good'] = example['sentence_good'].replace(\"hasn't\", \"has not\")\n",
    "    example['sentence_bad'] = example['sentence_bad'].replace(\"hasn't\", \"has not\")\n",
    "\n",
    "    example['sentence_good'] = example['sentence_good'].replace(\"haven't\", \"have not\")\n",
    "    example['sentence_bad'] = example['sentence_bad'].replace(\"haven't\", \"have not\")\n",
    "\n",
    "    example['sentence_good'] = example['sentence_good'].replace(\"hadn't\", \"had not\")\n",
    "    example['sentence_bad'] = example['sentence_bad'].replace(\"hadn't\", \"had not\")\n",
    "\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0cd3b6",
   "metadata": {},
   "source": [
    "The Subject-Verb Agreement processing step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb8fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed each subset (task) into the processing function\n",
    "def sva_process(raw_data):\n",
    "    data = {}\n",
    "    data['sentence_good'] = [] \n",
    "    data['sentence_bad'] = []\n",
    "    data['good_word'] = [] # the target word in good sentence (correct word)\n",
    "    data['bad_word'] = [] # the target word in bad sentence (wrong word)\n",
    "    data['target_index'] = [] # the [MASK] token which we want to predict, this is the difference between two sentences\n",
    "    \n",
    "    # the index of cue word, ex: for Subject-Verb agreement it's the subject word which help us to predict the verb correctly\n",
    "    data['cue_indices'] = [] \n",
    "    data['labels'] = []\n",
    "    # for each pair of data in the specific task\n",
    "    for example in raw_data:\n",
    "        # creating a spaCy document object by processing each sentence\n",
    "        doc_good = nlp(example['sentence_good'])\n",
    "        doc_bad = nlp(example['sentence_bad'])\n",
    "        \n",
    "        cue_index = -1\n",
    "        sentence_good = []\n",
    "        sentence_bad = []\n",
    "        # loop for number of tokens in the sentence\n",
    "        for i in range(len(doc_good)):\n",
    "            # add the str of each token to lists\n",
    "            sentence_good.append(doc_good[i].text)\n",
    "            sentence_bad.append(doc_bad[i].text)\n",
    "            # \"nsubj\" -> nominal subject: It's a grammatical dependency relation that indicates the noun phrase\n",
    "            # (or other nominal expression) that functions as the subject of the sentence.\n",
    "            if doc_good[i].dep_ == \"nsubj\" and cue_index == -1:\n",
    "                cue_index = i\n",
    "            if doc_good[i].text != doc_bad[i].text:\n",
    "                target_index = i\n",
    "                good_word = doc_good[i].text\n",
    "                bad_word = doc_bad[i].text\n",
    "                \n",
    "                tag = doc_good[target_index].tag_\n",
    "                if tag == 'VBZ':\n",
    "                    tag = 'singular'\n",
    "                elif tag == 'VBP':\n",
    "                    tag = 'plural'\n",
    "\n",
    "                # fix wrong tags of SpaCy\n",
    "                elif tag == \"NN\": # it's vice versa becuse spacy consideres verb as a noun, so those plural nouns are actually a singular verb e.g., works\n",
    "                    tag = \"plural\"\n",
    "                elif tag == \"NNS\":\n",
    "                    tag = \"singular\"\n",
    "                elif doc_good[target_index].tag_ not in ['VBZ', 'VBP'] and doc_bad[target_index].tag_ in ['VBZ', 'VBP']:\n",
    "                    if doc_bad[target_index].tag_ == \"VBZ\":\n",
    "                        tag = \"plural\"\n",
    "                    elif doc_bad[target_index].tag_ == \"VBP\":\n",
    "                        tag = \"singular\"\n",
    "\n",
    "                elif tag not in ['VBZ', 'VBP']: #correcting exceptions\n",
    "                    if doc_good[target_index].text in [\"was\", \"upsets\", \"hurts\", \"bores\", \"vanishes\", \"distracts\", \"kisses\", \"boycotts\", \"scares\"]:\n",
    "                        tag = \"singular\"\n",
    "                    elif doc_good[target_index].text in [\"were\", \"upset\", \"hurt\", \"bore\", \"vanish\", \"distract\", \"kiss\", \"boycott\", \"scare\"]:\n",
    "                        tag = \"plural\"\n",
    "                    else:\n",
    "                        print(doc_good[target_index].text, doc_bad[target_index].text)\n",
    "\n",
    "\n",
    "        if target_index == -1 or cue_index == -1:\n",
    "            continue\n",
    "        # in case the candidate [MASK] does not not consists of a single token (for the specific tokenizer)\n",
    "        if len(tokenizer_bert.tokenize(good_word)) > 1 or len(tokenizer_bert.tokenize(bad_word)) > 1:\n",
    "            continue\n",
    "\n",
    "        data['sentence_good'].append(sentence_good) # list of lists contains of tokens in each sentence parsed by spacy\n",
    "        data['sentence_bad'].append(sentence_bad)\n",
    "        data['target_index'].append(target_index) \n",
    "        data['cue_indices'].append([cue_index])\n",
    "        data['good_word'].append(good_word)\n",
    "        data['bad_word'].append(bad_word)\n",
    "        data['labels'].append(tag) # plurality/singularity of target word for the good sentence\n",
    "\n",
    "    return Dataset.from_dict(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3c777",
   "metadata": {},
   "source": [
    "The Determiner-Noun Agreement processing step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c27b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_process(raw_data):\n",
    "    data = {}\n",
    "    data['sentence_good'] = []\n",
    "    data['sentence_bad'] = []\n",
    "    data['good_word'] = []\n",
    "    data['bad_word'] = []\n",
    "    data['target_index'] = []\n",
    "    data['cue_indices'] = []\n",
    "    data['labels'] = []\n",
    "    for ex, example in enumerate(raw_data):\n",
    "        doc_good = nlp(example['sentence_good'])\n",
    "        doc_bad = nlp(example['sentence_bad'])\n",
    "        edges = [] # a list contains all edges in the graph of dependency tree\n",
    "        # iterate over words in each doc and find its children. Then create a tuple of index of the word and the indec of the child\n",
    "        for w in doc_good:\n",
    "            edges.extend([(w.i, child.i) for child in w.children])\n",
    "\n",
    "        target_index = -1\n",
    "        cue_index = -1\n",
    "        sentence_good = []\n",
    "        sentence_bad = []\n",
    "        for i in range(len(doc_good)):\n",
    "            sentence_good.append(doc_good[i].text)\n",
    "            sentence_bad.append(doc_bad[i].text)\n",
    "            if doc_good[i].text != doc_bad[i].text: # doc_good[i].dep_ == \"det\" and\n",
    "                target_index = i\n",
    "                good_word = doc_good[i].text\n",
    "                bad_word = doc_bad[i].text\n",
    "            \n",
    "            # Search over all edges in the sentence to find the edge which points at the target word (the determiner),\n",
    "            # so the parent of this edge is our cue word. (In dependency tree the NOUN points at DET)\n",
    "            for s, d in edges:\n",
    "                if d == target_index:\n",
    "                    cue_index = s\n",
    "                    break\n",
    "\n",
    "        if target_index == -1 or cue_index == -1:\n",
    "            print(ex)\n",
    "            continue\n",
    "        if len(tokenizer_bert.tokenize(good_word)) > 1 or len(tokenizer_bert.tokenize(bad_word)) > 1:\n",
    "            continue\n",
    "\n",
    "        if good_word in ['this', 'that']:\n",
    "            tag = \"singular\"\n",
    "        elif good_word in ['these', 'those']:\n",
    "            tag = \"plural\"\n",
    "        else:\n",
    "            print(good_word)\n",
    "\n",
    "        data['sentence_good'].append(sentence_good)\n",
    "        data['sentence_bad'].append(sentence_bad)\n",
    "        data['target_index'].append(target_index)\n",
    "        data['cue_indices'].append([cue_index])\n",
    "        data['good_word'].append(good_word)\n",
    "        data['bad_word'].append(bad_word)\n",
    "        data['labels'].append(tag)\n",
    "\n",
    "    return Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488cd760",
   "metadata": {},
   "source": [
    "The Anaphor Number Agreement processing step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc59db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_process(raw_data):\n",
    "    data = {}\n",
    "    data['sentence_good'] = []\n",
    "    data['sentence_bad'] = []\n",
    "    data['good_word'] = []\n",
    "    data['bad_word'] = []\n",
    "    data['target_index'] = []\n",
    "    data['cue_indices'] = []\n",
    "    data['labels'] = []\n",
    "    for ex, example in enumerate(raw_data):\n",
    "        doc_good = nlp(example['sentence_good'])\n",
    "        doc_bad = nlp(example['sentence_bad'])\n",
    "        if not doc_good._.has_coref:\n",
    "            continue\n",
    "        # coref_clusters returns all the coref clusters. Here we take the first cluster ([0]) and it contains of the\n",
    "        # [coref_1 (entity), coref_2]. So, the first word is our cue.\n",
    "        cue_words, _ = doc_good._.coref_clusters[0]\n",
    "        # The cue may have more than one word.\n",
    "        cue_words = cue_words.text.split(\" \")\n",
    "        target_index = -1\n",
    "        cue_indices = [] # In contrast with previous functions here we have a list of indices for cue\n",
    "        sentence_good = []\n",
    "        sentence_bad = []\n",
    "        for i in range(len(doc_good)):\n",
    "            sentence_good.append(doc_good[i].text)\n",
    "            sentence_bad.append(doc_bad[i].text)\n",
    "            if doc_good[i].text != doc_bad[i].text:\n",
    "                target_index = i\n",
    "                good_word = doc_good[i].text\n",
    "                bad_word = doc_bad[i].text\n",
    "\n",
    "            if doc_good[i].text in cue_words:\n",
    "                cue_indices.append(i)\n",
    "\n",
    "        if target_index == -1 or not cue_indices:\n",
    "            continue\n",
    "        if len(tokenizer_bert.tokenize(good_word)) > 1 or len(tokenizer_bert.tokenize(bad_word)) > 1:\n",
    "            continue\n",
    "\n",
    "        if good_word in ['itself', 'himself', 'herself']:\n",
    "            tag = \"singular\"\n",
    "        elif good_word in ['themselves']:\n",
    "            tag = \"plural\"\n",
    "        else:\n",
    "            print(good_word)\n",
    "\n",
    "        data['sentence_good'].append(sentence_good)\n",
    "        data['sentence_bad'].append(sentence_bad)\n",
    "        data['target_index'].append(target_index)\n",
    "        data['cue_indices'].append(cue_indices)\n",
    "        data['good_word'].append(good_word)\n",
    "        data['bad_word'].append(bad_word)\n",
    "        data['labels'].append(tag)\n",
    "\n",
    "    return Dataset.from_dict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a1b1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_UID = {\n",
    "    'anaphor_number_agreement': 'ana',\n",
    "    'determiner_noun_agreement_2': 'dna',\n",
    "    'determiner_noun_agreement_with_adj_2': 'dnaa',\n",
    "    'distractor_agreement_relational_noun': 'darn',\n",
    "    'regular_plural_subject_verb_agreement_1': 'rpsv',\n",
    "}\n",
    "\n",
    "UID_PROCESSOR = {\n",
    "    'ana': number_process,\n",
    "    'dna': det_process,\n",
    "    'dnaa': det_process,\n",
    "    'rpsv': sva_process,\n",
    "    'darn': sva_process,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c89ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12\n",
    "\n",
    "# Load Tokenizer\n",
    "# MODEL_NAME = \"bert\" # \"bert\", \"roberta\", \"electra\"\n",
    "MODEL_NAME = \"roberta\" # \"bert\", \"roberta\", \"electra\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH[MODEL_NAME])\n",
    "# tokenizer_bert = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base', use_fast=False)\n",
    "\n",
    "# Load spacy\n",
    "# nlp = spacy.load('en') # Loading the English Language Model\n",
    "# neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e170c16c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset blimp (/home/s_abbasi/.cache/huggingface/datasets/blimp/anaphor_number_agreement/0.1.0/0c65b833b8653dc81bbd517025f8248bcc8a94407cfc06a390abfd213d7cc13c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8a307f4f4c4e7a8b4e649adc4b856f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b066e2f6764569be77400394edfa24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset blimp (/home/s_abbasi/.cache/huggingface/datasets/blimp/determiner_noun_agreement_2/0.1.0/0c65b833b8653dc81bbd517025f8248bcc8a94407cfc06a390abfd213d7cc13c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4aee024d1704ab68cc4aa060f8a9e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b543eeea8a5d485586dd3f909c39a91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset blimp (/home/s_abbasi/.cache/huggingface/datasets/blimp/determiner_noun_agreement_with_adj_2/0.1.0/0c65b833b8653dc81bbd517025f8248bcc8a94407cfc06a390abfd213d7cc13c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf75a0091034354b4a77827a93f3644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd0d3cdc591448ea2651ccbe6d7b5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset blimp (/home/s_abbasi/.cache/huggingface/datasets/blimp/distractor_agreement_relational_noun/0.1.0/0c65b833b8653dc81bbd517025f8248bcc8a94407cfc06a390abfd213d7cc13c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8b02a31002479a96a9ab9af8ac9d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d41f2599950453fa4ddecf71d529461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long longs\n",
      "depart departs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset blimp (/home/s_abbasi/.cache/huggingface/datasets/blimp/regular_plural_subject_verb_agreement_1/0.1.0/0c65b833b8653dc81bbd517025f8248bcc8a94407cfc06a390abfd213d7cc13c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31b5bba04eb4d3790124e0799b48ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab5009afa24497c907b4472b4164173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heal heals\n"
     ]
    }
   ],
   "source": [
    "# for each task we load the train split of the data from hugging face\n",
    "for task, uid in TASK_UID.items():\n",
    "    raw_data = load_dataset(\"blimp\", task)['train']\n",
    "    raw_data = raw_data.map(expand_contraction_process) # expanding the contractions\n",
    "    \n",
    "    # performing processing by type of task\n",
    "    data = UID_PROCESSOR[uid](raw_data)\n",
    "    data = data.shuffle(seed=SEED)\n",
    "\n",
    "    if uid == \"rpsv\": # balancing class labels\n",
    "        plur_indices = np.where(np.array(data['labels']) == 'plural')[0]\n",
    "        sing_indices = np.where(np.array(data['labels']) == 'singular')[0]\n",
    "        sing_indices = np.random.choice(sing_indices, len(plur_indices))\n",
    "        plur_data = data.select(plur_indices)\n",
    "        sing_data = data.select(sing_indices)\n",
    "        data = concatenate_datasets([plur_data, sing_data])\n",
    "        data = data.shuffle(seed=SEED)\n",
    "\n",
    "    # aggregate datasets\n",
    "    if uid == \"ana\": # \"ana\" is the first uid, after that always the eslse statement will be run\n",
    "        number_dataset = data\n",
    "    else:\n",
    "        number_dataset = concatenate_datasets([number_dataset, data])\n",
    "        number_dataset = number_dataset.shuffle(seed=SEED)\n",
    "\n",
    "number_dataset = number_dataset.shuffle(seed=SEED)\n",
    "number_dataset = number_dataset.train_test_split(test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5877184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4a58921e0f4e9aa857dd32f454ad60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b1da48cf754b7ca78f394cf3a10176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_dataset.save_to_disk(f\"./BLIMP Dataset/{MODEL_NAME}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c600c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "ds = load_from_disk(f\"./BLIMP Dataset/{MODEL_NAME}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b5d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "1610 ana ['James', 'can', 'kiss', 'himself', '.'],\n",
    "1614 ana ['James', 'impressed', 'himself', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "305dcd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 ['A', 'sister', 'of', 'these', 'cashiers', 'is', 'slumping', 'over', '.']\n",
      "507 ['The', 'sister', 'of', 'the', 'Impressionists', 'appears', 'to', 'exit', 'this', 'lake', '.']\n",
      "1291 ['The', 'sister', 'of', 'doctors', 'writes', '.']\n",
      "1322 ['The', 'sister', 'of', 'these', 'dancers', 'has', 'swallowed', '.']\n",
      "1633 ['The', 'sister', 'of', 'a', 'lot', 'of', 'drivers', 'responds', '.']\n",
      "1712 ['The', 'sister', 'of', 'all', 'cashiers', 'forces', 'Andrea', \"'s\", 'best', 'friend', 'to', 'cry', '.']\n",
      "1905 ['The', 'sister', 'of', 'patients', 'sits', 'down', '.']\n",
      "2006 ['A', 'sister', 'of', 'all', 'girls', 'does', 'not', 'implore', 'Diana', 'to', 'fall', 'asleep', '.']\n",
      "2100 ['The', 'sister', 'of', 'those', 'men', 'does', 'not', 'scare', 'Grace', '.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ds['train'])):\n",
    "    if 'sister' in ds['train'][i]['sentence_good']:\n",
    "        print(i, ds['train'][i]['sentence_good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa1c3c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence_good', 'sentence_bad', 'good_word', 'bad_word', 'target_index', 'cue_indices', 'labels'],\n",
       "        num_rows: 2137\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence_good', 'sentence_bad', 'good_word', 'bad_word', 'target_index', 'cue_indices', 'labels'],\n",
       "        num_rows: 2137\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "339109de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(number_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4ddef01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_good': ['The',\n",
       "  'granddaughters',\n",
       "  'of',\n",
       "  'Thomas',\n",
       "  'have',\n",
       "  'messed',\n",
       "  'up',\n",
       "  'this',\n",
       "  'high',\n",
       "  'school',\n",
       "  '.'],\n",
       " 'sentence_bad': ['The',\n",
       "  'granddaughters',\n",
       "  'of',\n",
       "  'Thomas',\n",
       "  'has',\n",
       "  'messed',\n",
       "  'up',\n",
       "  'this',\n",
       "  'high',\n",
       "  'school',\n",
       "  '.'],\n",
       " 'good_word': 'have',\n",
       " 'bad_word': 'has',\n",
       " 'target_index': 4,\n",
       " 'cue_indices': [1],\n",
       " 'labels': 'plural'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23808b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_good': ['Every',\n",
       "  'pedestrian',\n",
       "  'can',\n",
       "  'not',\n",
       "  'hurt',\n",
       "  'that',\n",
       "  'unconvinced',\n",
       "  'girl',\n",
       "  '.'],\n",
       " 'sentence_bad': ['Every',\n",
       "  'pedestrian',\n",
       "  'can',\n",
       "  'not',\n",
       "  'hurt',\n",
       "  'those',\n",
       "  'unconvinced',\n",
       "  'girl',\n",
       "  '.'],\n",
       " 'good_word': 'that',\n",
       " 'bad_word': 'those',\n",
       " 'target_index': 5,\n",
       " 'cue_indices': [7],\n",
       " 'labels': 'singular'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7489c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sina_pt_gpu",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
