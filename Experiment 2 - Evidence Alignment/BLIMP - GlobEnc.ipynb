{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a74fde2",
   "metadata": {},
   "source": [
    "# The Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915bd5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use the GPU: Quadro RTX 8000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(f\"cuda:{0}\")\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4aa92e",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d5bed97f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Cloning into 'GlobEnc'...\n",
      "remote: Enumerating objects: 161, done.\u001b[K\n",
      "remote: Counting objects: 100% (161/161), done.\u001b[K\n",
      "remote: Compressing objects: 100% (120/120), done.\u001b[K\n",
      "remote: Total 161 (delta 88), reused 86 (delta 35), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (161/161), 9.04 MiB | 3.29 MiB/s, done.\n",
      "Resolving deltas: 100% (88/88), done.\n"
     ]
    }
   ],
   "source": [
    "# ! git clone https://github.com/mohsenfayyaz/GlobEnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "745d26f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from GlobEnc.src.modeling.modeling_bert import BertForSequenceClassification, BertForMaskedLM\n",
    "from GlobEnc.src.modeling.modeling_electra import ElectraForSequenceClassification\n",
    "from GlobEnc.src.modeling.modeling_roberta import RobertaForSequenceClassification, RobertaForMaskedLM\n",
    "from GlobEnc.src.attention_rollout import AttentionRollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edd6f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NUM_LABELS = {\n",
    "    \"ana\": 2,\n",
    "    \"dna\": 2,\n",
    "    \"dnaa\": 2,\n",
    "    \"rpsv\": 2,\n",
    "    \"darn\": 2,\n",
    "    \"NA\": 2,\n",
    "}\n",
    "\n",
    "blimp_to_label = {\n",
    "    'singular': 0,\n",
    "    'plural': 1,\n",
    "}\n",
    "\n",
    "MODEL_PATH = {\n",
    "    'bert': 'bert-base-uncased',\n",
    "    'roberta': 'roberta-base',\n",
    "    'electra': 'google/electra-base-generator',\n",
    "    'deberta': 'microsoft/deberta-v3-base'\n",
    "}\n",
    "\n",
    "BLIMP_TASKS = [\n",
    "    \"ana\",\n",
    "    'dna',\n",
    "    \"dnaa\",\n",
    "    \"rpsv\",\n",
    "    \"darn\",\n",
    "    \"NA\",\n",
    "]\n",
    "\n",
    "def blimp_to_features(data, tokenizer, max_length, input_masking, mlm):\n",
    "    all_features = []\n",
    "    for example in data:\n",
    "        text = example['sentence_good']\n",
    "        tokens = []\n",
    "        cue_indices = []\n",
    "        # token to id\n",
    "        for w_ind, word in enumerate(text):\n",
    "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "            if w_ind in example['cue_indices']:\n",
    "                cue_indices.append(len(tokens))\n",
    "            if w_ind == example['target_index']:\n",
    "                target_index = len(tokens)\n",
    "            tokens.extend(ids)\n",
    "        \n",
    "        tokens = [tokenizer.cls_token_id] + tokens + [tokenizer.sep_token_id]\n",
    "        cue_indices = [x+1 for x in cue_indices] # 'cause of adding cls\n",
    "        target_index += 1 # 'cause of adding cls\n",
    "        if input_masking:\n",
    "            tokens[target_index] = tokenizer.mask_token_id\n",
    "\n",
    "        # padding\n",
    "        length = len(tokens)\n",
    "        inputs = {}\n",
    "        inputs['input_ids'] = tokens if max_length is None else tokens + [tokenizer.pad_token_id]*(max_length - length)\n",
    "        inputs['input_ids'] = torch.tensor(inputs['input_ids']).to(device)\n",
    "        inputs['attention_mask'] = [1]*length if max_length is None else [1]*length + [0]*(max_length - length)\n",
    "        inputs['attention_mask'] = torch.tensor(inputs['attention_mask']).to(device)\n",
    "        inputs['token_type_ids'] = torch.tensor([0]*length if max_length is None else [0]*max_length).to(device)\n",
    "        inputs['target_index'] = target_index\n",
    "        \n",
    "        # As a 2d tensor, we need all rows to have the same length. So, we add -1 to the end of each list.\n",
    "        inputs['cue_indices'] = cue_indices + (10 - len(cue_indices)) * [-1]\n",
    "\n",
    "        all_features.append(inputs)\n",
    "    return all_features[0] if len(all_features) == 1 else all_features\n",
    "\n",
    "PREPROCESS_FUNC = {\n",
    "    'ana': blimp_to_features,\n",
    "    'dna': blimp_to_features,\n",
    "    'dnaa': blimp_to_features,\n",
    "    'rpsv': blimp_to_features,\n",
    "    'darn': blimp_to_features,\n",
    "    'NA': blimp_to_features,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b78354ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at BLIMP Dataset/roberta/train/cache-3c138be14de5eff0.arrow\n"
     ]
    }
   ],
   "source": [
    "SELECTED_GPU = 0\n",
    "MODEL_NAME = 'roberta'\n",
    "FIXED = False\n",
    "TASK = \"NA\"\n",
    "MAX_LENGTH = 32\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "PER_DEVICE_BATCH_SIZE = 1\n",
    "\n",
    "INPUT_MASKING = True\n",
    "MLM = True\n",
    "LEARNING_RATE = 3e-5\n",
    "LR_SCHEDULER_TYPE = \"linear\" \n",
    "WARMUP_RATIO = 0.1\n",
    "SEED = 42\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    load_from_disk,\n",
    "    load_metric,\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "set_seed(SEED)\n",
    "\n",
    "# Load Dataset\n",
    "if TASK in BLIMP_TASKS:\n",
    "    data_path = f\"./BLIMP Dataset/{MODEL_NAME}/\"\n",
    "    data = load_from_disk(data_path)\n",
    "    train_data = data['train']\n",
    "    eval_data = data['test']\n",
    "else:\n",
    "    print(\"Not implemented yet!\")\n",
    "    exit()\n",
    "\n",
    "train_data = train_data.shuffle(SEED)\n",
    "num_labels = NUM_LABELS[TASK]\n",
    "\n",
    "# Download Tokenizer & Model\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH[MODEL_NAME], num_labels=num_labels)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH[MODEL_NAME])  \n",
    "\n",
    "if MODEL_NAME == \"bert\":\n",
    "    model = BertForMaskedLM.from_pretrained(MODEL_PATH[MODEL_NAME], config=config)\n",
    "elif MODEL_NAME == \"roberta\":\n",
    "    model = RobertaForMaskedLM.from_pretrained(MODEL_PATH[MODEL_NAME], config=config)\n",
    "# elif MODEL_NAME == \"electra\":\n",
    "#     model = ElectraForMaskedLM.from_pretrained(MODEL_PATH[MODEL_NAME], config=config)\n",
    "else:\n",
    "    print(\"model doesn't exist\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Preprocessing\n",
    "train_dataset = PREPROCESS_FUNC[TASK](train_data, tokenizer, MAX_LENGTH, input_masking=INPUT_MASKING, mlm=MLM)\n",
    "eval_dataset = PREPROCESS_FUNC[TASK](eval_data, tokenizer, MAX_LENGTH, input_masking=INPUT_MASKING, mlm=MLM)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn= default_data_collator, batch_size=PER_DEVICE_BATCH_SIZE)\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn= default_data_collator, batch_size=PER_DEVICE_BATCH_SIZE)\n",
    "\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "max_train_steps = NUM_TRAIN_EPOCHS * num_update_steps_per_epoch \n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = get_scheduler(\n",
    "        name=LR_SCHEDULER_TYPE,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=WARMUP_RATIO * max_train_steps,\n",
    "        num_training_steps=max_train_steps,\n",
    "    )\n",
    "\n",
    "# metric & Loss\n",
    "metric = load_metric(\"accuracy\")\n",
    "loss_fct = CrossEntropyLoss()\n",
    "\n",
    "tag = \"forseqclassification_\"\n",
    "tag += \"pretrained\" if FIXED else \"finetuned\" \n",
    "if MLM:\n",
    "    tag += \"_MLM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436975cf",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b596113b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'{MODEL_NAME}_full_{tag}_epoch{NUM_TRAIN_EPOCHS}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in eval_dataloader:\n",
    "    if MLM:\n",
    "        good_token_id = batch.pop('good_token_id').to(device)\n",
    "        bad_token_id = batch.pop('bad_token_id').to(device)\n",
    "    batch.pop('cue_indices').to(device)\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    if MLM:\n",
    "        good_logits = logits[torch.arange(logits.size(0)), good_token_id]\n",
    "        bad_logits = logits[torch.arange(logits.size(0)), bad_token_id]\n",
    "        logits_of_interest = torch.stack([good_logits, bad_logits], dim=1)\n",
    "        labels = torch.zeros(logits_of_interest.shape[0], dtype=torch.int64, device=device)\n",
    "        predictions = torch.argmax(logits_of_interest, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=labels)\n",
    "    else:\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch['labels'])\n",
    "\n",
    "eval_metric = metric.compute()\n",
    "print(f\"epoch {epoch}: {eval_metric}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6443538c",
   "metadata": {},
   "source": [
    "# GlobEnc\n",
    "\n",
    "The implementations were sourced from https://github.com/mohsenfayyaz/GlobEnc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3abc2fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73db1bcf2c4e4fb1a44d6bc9324be52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Computing GlobEnc importance scores per instance for all layers\n",
    "GE_per_layer_scores = list()\n",
    "\n",
    "for batch_sample in tqdm(eval_dataloader):\n",
    "    \n",
    "    batch_lengths = batch_sample['attention_mask'].sum(-1)\n",
    "    logits, attentions, norms = model(batch_sample['input_ids'].to(device), batch_sample['attention_mask'].to(device), batch_sample['token_type_ids'].to(device), output_attentions=True, output_norms=True, return_dict=False)\n",
    "    num_layers = len(attentions)\n",
    "    norm_nenc = torch.stack([norms[i][4] for i in range(num_layers)]).squeeze().cpu().detach().numpy()\n",
    "    \n",
    "    GE_per_layer_scores.append(norm_nenc[:, :batch_lengths.item(), :batch_lengths.item()])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f53e2",
   "metadata": {},
   "source": [
    "# Alignment Metrics\n",
    "\n",
    "Here, we compute Dot Product and Average Precision. At first, let's define a method to compute Average Precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394acf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the tensors from -1 padding\n",
    "def CI_cleaner(CI):\n",
    "    first_pad_index = torch.where(CI == -1)[0][0].item() # We have used -1 as paddings of CIs\n",
    "    return CI[:first_pad_index]\n",
    "\n",
    "# Calculating Precision\n",
    "def precision(TP, FP):\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "# Calculating Recall\n",
    "def recall(TP, FN):\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "# Calculating Average Precision\n",
    "def avg_precision(topk, CI):\n",
    "    R_base = 0 # The starting recall before the first round\n",
    "    AP, TP, FP, FN = 0, 0, 0, len(CI)\n",
    "    previous_recall = R_base\n",
    "    for i in range(len(topk)):\n",
    "        if topk[i] in CI:\n",
    "            TP += 1\n",
    "            FN -= 1\n",
    "        else:\n",
    "            FP += 1\n",
    "\n",
    "        AP += (recall(TP, FN) -  previous_recall) * precision(TP, FP)\n",
    "        previous_recall = recall(TP, FN)\n",
    "\n",
    "    return AP\n",
    "\n",
    "topk = torch.tensor([1, 0, 3, 4, 2])\n",
    "CI = torch.tensor([3, -1, -1, -1])\n",
    "# CI = CI_cleaner(CI)\n",
    "# print(avg_precision(topk, CI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4213b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diagram_layers = range(12)\n",
    "APs_GE = dict()\n",
    "\n",
    "for layer in diagram_layers:\n",
    "    APs_GE[f'layer{layer}'] = list()\n",
    "\n",
    "sum_GE_scores = 0\n",
    "\n",
    "model.eval()\n",
    "for i, batch_sample in enumerate(tqdm(eval_dataloader)):\n",
    "    \n",
    "    CI = CI_cleaner(torch.tensor(batch_sample['cue_indices'][0])) # [0]: because we only have one sample in each batch\n",
    "    \n",
    "    ### Average Precision\n",
    "    batch_lengths = batch_sample['attention_mask'].sum(axis=-1)\n",
    "    mask_index = batch_sample['target_index'] # mask_index = target_index\n",
    "\n",
    "    # The contribution of each token in the sequence in building the rep. of target token for different layers\n",
    "    GE_importance = GE_per_layer_scores[i][:, batch_sample['target_index']] # shape: [12, seq_len]\n",
    "    # Convert to torch tensor form numpy ndarray\n",
    "    GE_importance = torch.from_numpy(GE_importance)\n",
    "    # batch_lengths[0]: because we only have one sample in each batch\n",
    "    GE_importance_topk = torch.topk(GE_importance, k=batch_lengths[0], largest=True, dim=1).indices\n",
    "    \n",
    "    ### excluding mask_index\n",
    "    mask_index_tensor = torch.full_like(GE_importance_topk, mask_index.item())\n",
    "    # Create a mask that is True for elements not equal to mask_index\n",
    "    mask = GE_importance_topk != mask_index_tensor\n",
    "    # # Apply the mask to exclude mask_index\n",
    "    GE_importance_topk_filtered = GE_importance_topk[mask].view(GE_importance_topk.size(0), -1)\n",
    "\n",
    "    for layer, layer_importance in enumerate(GE_importance_topk_filtered):\n",
    "        APs_GE[f'layer{layer}'].append(avg_precision(layer_importance, CI))\n",
    "        \n",
    "    ### Dot Product\n",
    "    # Remove mask_index and then normalize the scores.\n",
    "    GE_scores = GE_per_layer_scores[i][:, batch_sample['target_index']]\n",
    "    GE_scores = torch.from_numpy(GE_scores)\n",
    "    GE_scores = torch.concat((GE_scores[:, :mask_index.item()], GE_scores[:, mask_index.item() + 1:]), dim=1)\n",
    "    GE_scores = GE_scores / GE_scores.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    if CI[-1] > mask_index:\n",
    "        CI_scores = GE_scores[:, CI - 1] # Because of the removed mask_index\n",
    "    else:\n",
    "        CI_scores = GE_scores[:, CI]\n",
    "\n",
    "    # In case there are more than one cue indices (i.e. evidence)\n",
    "    if CI.shape[0] > 1:\n",
    "        CI_scores = CI_scores.sum(axis=1, keepdim=True)\n",
    "        \n",
    "    sum_GE_scores += CI_scores\n",
    "\n",
    "print(\"### Dot Product ###\")\n",
    "print(sum_GE_scores / len(eval_dataloader))\n",
    "\n",
    "print(\"### Average Precision ###\")\n",
    "temp_list = list()\n",
    "for layer in diagram_layers:\n",
    "    temp_list.append(sum(APs_GE[f'layer{layer}']) / len(APs_GE[f'layer{layer}']))\n",
    "\n",
    "print(temp_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5823e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GlobEnc pre-trained\n",
    "### Dot Product ###\n",
    "[0.1049, 0.1069, 0.1134, 0.1020, 0.1128, 0.1199, 0.1050, 0.1192, 0.1213, 0.1047, 0.1144, 0.1303]\n",
    "### Average Precision ###\n",
    "[0.29353671136159953, 0.3428355231122479, 0.3661429698928716, 0.2674434673360707, 0.31858515302635293, 0.34495581585853935,\n",
    " 0.2842268998074423, 0.34539070185299914, 0.3265184481704264, 0.2922642782829635, 0.3191821681466628, 0.352063390024211]\n",
    "\n",
    "# GlobEnc Finetuned\n",
    "### Dot Product ###\n",
    "[0.1055, 0.1067, 0.1150, 0.1006, 0.1124, 0.1160, 0.1051, 0.1037, 0.1312, 0.0940, 0.1273, 0.0919]\n",
    "### Average Precision ###\n",
    "[0.29497986424836653, 0.34115687026569663, 0.3699041272101137, 0.2659901258731381, 0.30648929408720155, 0.32037714502900505,\n",
    " 0.28131746623844195, 0.28891366741031027, 0.36654652620175787, 0.2702167037045549, 0.3625181160677095, 0.23063080852453038]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af824b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sina_pt_gpu",
   "language": "python",
   "name": "sina_pt_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
