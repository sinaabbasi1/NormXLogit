{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a74fde2",
   "metadata": {},
   "source": [
    "# The Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915bd5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use the GPU: Quadro RTX 8000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(f\"cuda:{0}\")\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4aa92e",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fbeef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NUM_LABELS = {\n",
    "    \"ana\": 2,\n",
    "    \"dna\": 2,\n",
    "    \"dnaa\": 2,\n",
    "    \"rpsv\": 2,\n",
    "    \"darn\": 2,\n",
    "    \"NA\": 2,\n",
    "}\n",
    "\n",
    "blimp_to_label = {\n",
    "    'singular': 0,\n",
    "    'plural': 1,\n",
    "}\n",
    "\n",
    "MODEL_PATH = {\n",
    "    'bert': 'bert-base-uncased',\n",
    "    'roberta': 'roberta-base',\n",
    "    'electra': 'google/electra-base-generator',\n",
    "    'deberta': 'microsoft/deberta-v3-base'\n",
    "}\n",
    "\n",
    "BLIMP_TASKS = [\n",
    "    \"ana\",\n",
    "    'dna',\n",
    "    \"dnaa\",\n",
    "    \"rpsv\",\n",
    "    \"darn\",\n",
    "    \"NA\",\n",
    "]\n",
    "\n",
    "def blimp_to_features(data, tokenizer, max_length, input_masking, mlm):\n",
    "    all_features = []\n",
    "    for example in data:\n",
    "        text = example['sentence_good']\n",
    "        tokens = []\n",
    "        cue_indices = []\n",
    "        # token to id\n",
    "        for w_ind, word in enumerate(text):\n",
    "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "            if w_ind in example['cue_indices']:\n",
    "                cue_indices.append(len(tokens))\n",
    "            if w_ind == example['target_index']:\n",
    "                target_index = len(tokens)\n",
    "            tokens.extend(ids)\n",
    "        \n",
    "        tokens = [tokenizer.cls_token_id] + tokens + [tokenizer.sep_token_id]\n",
    "        cue_indices = [x+1 for x in cue_indices] # 'cause of adding cls\n",
    "        target_index += 1 # 'cause of adding cls\n",
    "        if input_masking:\n",
    "            tokens[target_index] = tokenizer.mask_token_id\n",
    "\n",
    "        # padding\n",
    "        length = len(tokens)\n",
    "        inputs = {}\n",
    "        inputs['input_ids'] = tokens if max_length is None else tokens + [tokenizer.pad_token_id]*(max_length - length)\n",
    "        inputs['input_ids'] = torch.tensor(inputs['input_ids']).to(device) # .unsqueeze(0)\n",
    "        inputs['attention_mask'] = [1]*length if max_length is None else [1]*length + [0]*(max_length - length)\n",
    "        inputs['attention_mask'] = torch.tensor(inputs['attention_mask']).to(device) # .unsqueeze(0).\n",
    "        # inputs['token_type_ids'] = [0]*length if max_length is None else [0]*max_length\n",
    "        inputs['target_index'] = target_index\n",
    "\n",
    "        # As a 2d tensor, we need all rows to have the same length. So, we add -1 to the end of each list.\n",
    "        inputs['cue_indices'] = cue_indices + (10 - len(cue_indices)) * [-1]\n",
    "\n",
    "        all_features.append(inputs)\n",
    "    return all_features[0] if len(all_features) == 1 else all_features\n",
    "\n",
    "PREPROCESS_FUNC = {\n",
    "    'ana': blimp_to_features,\n",
    "    'dna': blimp_to_features,\n",
    "    'dnaa': blimp_to_features,\n",
    "    'rpsv': blimp_to_features,\n",
    "    'darn': blimp_to_features,\n",
    "    'NA': blimp_to_features,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c46766",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from src.utils_contributions import *\n",
    "import torch.nn.functional as F\n",
    "from src.contributions import ModelWrapper, ClassificationModelWrapperCaptum, interpret_sentence, occlusion\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989ca5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = 'test'\n",
    "MODEL_NAME = 'roberta'\n",
    "\n",
    "data_path = f\"./BLIMP Dataset/{MODEL_NAME}/\"\n",
    "dataset = load_from_disk(data_path)[SPLIT]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH[MODEL_NAME])\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH[MODEL_NAME], num_labels=2)\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_PATH[MODEL_NAME], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f98e87d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./roberta_full_forseqclassification_finetuned_MLM_epoch1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f892916",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_GPU = 0\n",
    "MODEL_NAME = 'roberta'\n",
    "FIXED = False\n",
    "TASK = \"NA\"\n",
    "MAX_LENGTH = None\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "PER_DEVICE_BATCH_SIZE = 1\n",
    "\n",
    "INPUT_MASKING = True\n",
    "MLM = True\n",
    "LEARNING_RATE = 3e-5\n",
    "LR_SCHEDULER_TYPE = \"linear\" \n",
    "WARMUP_RATIO = 0.1\n",
    "SEED = 42\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "eval_dataset = PREPROCESS_FUNC[TASK](dataset, tokenizer, MAX_LENGTH, input_masking=INPUT_MASKING, mlm=MLM)\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=PER_DEVICE_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c615b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914cac70",
   "metadata": {},
   "source": [
    "# ALTI\n",
    "\n",
    "The implementations were sourced from https://github.com/mt-upc/transformer-contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3954af12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc086d76bc5b424490dc3b8b95ab85bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "keys_to_select = ['input_ids', 'attention_mask']\n",
    "model_wrapped = ModelWrapper(model)\n",
    "\n",
    "ALTI_per_layer_scores = list()\n",
    "for batch_sample in tqdm(eval_dataloader):\n",
    "    \n",
    "    subset_dict = {key: batch_sample[key] for key in keys_to_select if key in batch_sample}\n",
    "    pt_batch = subset_dict\n",
    "    \n",
    "    prediction_scores, hidden_states, attentions, contributions_data = model_wrapped(pt_batch)\n",
    "    probs = torch.nn.functional.softmax(prediction_scores, dim=-1)\n",
    "    pred_ind = torch.argmax(probs)\n",
    "    pred = torch.max(probs)\n",
    "    \n",
    "    _attentions = [att.detach().cpu().numpy() for att in attentions]\n",
    "    attentions_mat = np.asarray(_attentions)[:,0] # (num_layers,num_heads,src_len,src_len)\n",
    "    att_mat_sum_heads = attentions_mat.sum(axis=1) / attentions_mat.shape[1]\n",
    "    normalized_model_norms = normalize_contributions(contributions_data['transformed_vectors_norm'],scaling='sum_one')\n",
    "    resultant_norm = resultants_norm = torch.norm(torch.squeeze(contributions_data['resultants']),p=1,dim=-1)\n",
    "    # ALTI Requires scaling = min_sum\n",
    "    normalized_contributions = normalize_contributions(contributions_data['contributions'], scaling='min_sum', resultant_norm=resultant_norm)\n",
    "    \n",
    "    ALTI_per_layer_scores.append(normalized_contributions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f53e2",
   "metadata": {},
   "source": [
    "# Alignment Metrics\n",
    "\n",
    "Here, we compute Dot Product and Average Precision. At first, let's define a method to compute Average Precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6603120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the tensors from -1 padding\n",
    "def CI_cleaner(CI):\n",
    "    first_pad_index = torch.where(CI == -1)[0][0].item() # We have used -1 as paddings of CIs\n",
    "    return CI[:first_pad_index]\n",
    "\n",
    "# Calculating Precision\n",
    "def precision(TP, FP):\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "# Calculating Recall\n",
    "def recall(TP, FN):\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "# Calculating Average Precision\n",
    "def avg_precision(topk, CI):\n",
    "    R_base = 0 # The starting recall before the first round\n",
    "    AP, TP, FP, FN = 0, 0, 0, len(CI)\n",
    "    previous_recall = R_base\n",
    "    for i in range(len(topk)):\n",
    "        if topk[i] in CI:\n",
    "            TP += 1\n",
    "            FN -= 1\n",
    "        else:\n",
    "            FP += 1\n",
    "\n",
    "        AP += (recall(TP, FN) -  previous_recall) * precision(TP, FP)\n",
    "        previous_recall = recall(TP, FN)\n",
    "\n",
    "    return AP\n",
    "\n",
    "topk = torch.tensor([1, 0, 3, 4, 2])\n",
    "CI = torch.tensor([3, -1, -1, -1])\n",
    "# CI = CI_cleaner(CI)\n",
    "# print(avg_precision(topk, CI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68588722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052615eafa514e96b3b4051743a1152f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Dot Product ###\n",
      "tensor([[0.1008],\n",
      "        [0.1513],\n",
      "        [0.1387],\n",
      "        [0.1094],\n",
      "        [0.1243],\n",
      "        [0.1329],\n",
      "        [0.1418],\n",
      "        [0.1130],\n",
      "        [0.1555],\n",
      "        [0.1157],\n",
      "        [0.1642],\n",
      "        [0.1291]])\n",
      "### Average Precision ###\n",
      "[0.25378783982724146, 0.3765116972267928, 0.384735210937058, 0.3131666564541012, 0.32833106186536964, 0.3317546323066029, 0.35110838518617177, 0.30322958040349385, 0.4014851593655965, 0.32560539946549477, 0.4042661331723848, 0.3256675418105456]\n"
     ]
    }
   ],
   "source": [
    "diagram_layers = range(12)\n",
    "APs_ALTI = dict()\n",
    "\n",
    "for layer in diagram_layers:\n",
    "    APs_ALTI[f'layer{layer}'] = list()\n",
    "\n",
    "sum_ALTI_scores = 0\n",
    "\n",
    "model.eval()\n",
    "for i, batch_sample in enumerate(tqdm(eval_dataset)):\n",
    "    \n",
    "    CI = CI_cleaner(torch.tensor(batch_sample['cue_indices'])) # [0]: because we only have one sample in each batch\n",
    "    \n",
    "    ### Average Precision\n",
    "    batch_lengths = batch_sample['attention_mask'].sum(axis=-1)\n",
    "    mask_index = batch_sample['target_index'] # mask_index = target_index\n",
    "\n",
    "    # The contribution of each token in the sequence in building the rep. of target token for different layers\n",
    "    ALTI_importance = ALTI_per_layer_scores[i][:, batch_sample['target_index']] # shape: [12, seq_len]\n",
    "    # Convert to torch tensor form numpy ndarray\n",
    "#     ALTI_importance = torch.from_numpy(ALTI_importance)\n",
    "    # batch_lengths[0]: because we only have one sample in each batch\n",
    "    ALTI_importance_topk = torch.topk(ALTI_importance, k=batch_lengths.item(), largest=True, dim=1).indices\n",
    "    \n",
    "    ### excluding mask_index\n",
    "    mask_index_tensor = torch.full_like(ALTI_importance_topk, mask_index)\n",
    "    # Create a mask that is True for elements not equal to mask_index\n",
    "    mask = ALTI_importance_topk != mask_index_tensor\n",
    "    # # Apply the mask to exclude mask_index\n",
    "    ALTI_importance_topk_filtered = ALTI_importance_topk[mask].view(ALTI_importance_topk.size(0), -1)\n",
    "\n",
    "    for layer, layer_importance in enumerate(ALTI_importance_topk_filtered):\n",
    "        APs_ALTI[f'layer{layer}'].append(avg_precision(layer_importance, CI))\n",
    "        \n",
    "    ### Dot Product\n",
    "    # Remove mask_index and then normalize the scores.\n",
    "    ALTI_scores = ALTI_per_layer_scores[i][:, batch_sample['target_index']]\n",
    "#     ALTI_scores = torch.from_numpy(ALTI_scores)\n",
    "    ALTI_scores = torch.concat((ALTI_scores[:, :mask_index], ALTI_scores[:, mask_index + 1:]), dim=1)\n",
    "    ALTI_scores = ALTI_scores / ALTI_scores.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    if CI[-1] > mask_index:\n",
    "        CI_scores = ALTI_scores[:, CI - 1] # Because of the removed mask_index\n",
    "    else:\n",
    "        CI_scores = ALTI_scores[:, CI]\n",
    "    \n",
    "    # In case there are more than one cue indices (i.e. evidence)\n",
    "    if CI.shape[0] > 1:\n",
    "        CI_scores = CI_scores.sum(axis=1, keepdim=True)\n",
    "        \n",
    "    sum_ALTI_scores += CI_scores\n",
    "\n",
    "print(\"### Dot Product ###\")\n",
    "print(sum_ALTI_scores / len(eval_dataset))\n",
    "\n",
    "print(\"### Average Precision ###\")\n",
    "temp_list = list()\n",
    "for layer in diagram_layers:\n",
    "    temp_list.append(sum(APs_ALTI[f'layer{layer}']) / len(APs_ALTI[f'layer{layer}']))\n",
    "\n",
    "print(temp_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1677fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa pre-trained\n",
    "\n",
    "### Dot Product ###\n",
    "[0.1000, 0.1535, 0.1346, 0.1101, 0.1351, 0.1429, 0.1394, 0.1363, 0.1353, 0.1239, 0.1443, 0.1748]\n",
    "### Average Precision ###\n",
    "[0.24847647621734462, 0.3781740227312561, 0.38813648741068457, 0.30460218191646077, 0.37426771660807046, 0.374563165041807,\n",
    " 0.3504273337748207, 0.3617342698439846, 0.3520740295961088, 0.32868386991179194, 0.37172958476457274, 0.39374281351833057]\n",
    "\n",
    "# RoBERTa fine-tuned\n",
    "\n",
    "### Dot Product ###\n",
    "[0.1008, 0.1513, 0.1387, 0.1094, 0.1243, 0.1329, 0.1418, 0.1130, 0.1555, 0.1157, 0.1642, 0.1291]\n",
    "### Average Precision ###\n",
    "[0.25378783982724146, 0.37580808697834556, 0.384735210937058, 0.3131100441352606, 0.32833106186536964, 0.3317546323066029,\n",
    " 0.35110838518617177, 0.30322958040349385, 0.4014851593655965, 0.3255487871466542, 0.4042661331723848, 0.3256675418105456]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sina_pt_gpu",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
