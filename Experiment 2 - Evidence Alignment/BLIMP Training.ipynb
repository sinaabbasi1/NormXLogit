{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d9d87f7",
   "metadata": {},
   "source": [
    "**Note:** We used the code of Value Zeroing method from https://github.com/hmohebbi/ValueZeroing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a74fde2",
   "metadata": {},
   "source": [
    "# The Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915bd5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use the GPU: Quadro RTX 8000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(f\"cuda:{0}\")\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4aa92e",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b22eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NUM_LABELS = {\n",
    "    \"ana\": 2,\n",
    "    \"dna\": 2,\n",
    "    \"dnaa\": 2,\n",
    "    \"rpsv\": 2,\n",
    "    \"darn\": 2,\n",
    "    \"NA\": 2,\n",
    "}\n",
    "\n",
    "blimp_to_label = {\n",
    "    'singular': 0,\n",
    "    'plural': 1,\n",
    "}\n",
    "\n",
    "MODEL_PATH = {\n",
    "    'bert': 'bert-base-uncased',\n",
    "    'roberta': 'roberta-base',\n",
    "    'electra': 'google/electra-base-generator',\n",
    "    'deberta': 'microsoft/deberta-v3-base'\n",
    "}\n",
    "\n",
    "BLIMP_TASKS = [\n",
    "    \"ana\",\n",
    "    'dna',\n",
    "    \"dnaa\",\n",
    "    \"rpsv\",\n",
    "    \"darn\",\n",
    "    \"NA\",\n",
    "]\n",
    "\n",
    "def blimp_to_features(data, tokenizer, max_length, input_masking, mlm):\n",
    "    all_features = []\n",
    "    for example in data:\n",
    "        text = example['sentence_good']\n",
    "        tokens = []\n",
    "        cue_indices = []\n",
    "        # token to id\n",
    "        for w_ind, word in enumerate(text):\n",
    "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "            if w_ind in example['cue_indices']:\n",
    "                cue_indices.append(len(tokens))\n",
    "            if w_ind == example['target_index']:\n",
    "                target_index = len(tokens)\n",
    "            tokens.extend(ids)\n",
    "        \n",
    "        tokens = [tokenizer.cls_token_id] + tokens + [tokenizer.sep_token_id]\n",
    "        cue_indices = [x+1 for x in cue_indices] # 'cause of adding cls\n",
    "        target_index += 1 # 'cause of adding cls\n",
    "        if input_masking:\n",
    "            tokens[target_index] = tokenizer.mask_token_id\n",
    "\n",
    "        # padding\n",
    "        length = len(tokens)\n",
    "        inputs = {}\n",
    "        inputs['input_ids'] = tokens if max_length is None else tokens + [tokenizer.pad_token_id]*(max_length - length)\n",
    "        inputs['attention_mask'] = [1]*length if max_length is None else [1]*length + [0]*(max_length - length)\n",
    "        inputs['token_type_ids'] = [0]*length if max_length is None else [0]*max_length\n",
    "        inputs['target_index'] = target_index\n",
    "        inputs['labels'] = tokenizer.convert_tokens_to_ids(example['good_word']) if mlm else blimp_to_label[example['labels']]\n",
    "        inputs['good_token_id'] = tokenizer.convert_tokens_to_ids(example['good_word'])\n",
    "        inputs['bad_token_id'] = tokenizer.convert_tokens_to_ids(example['bad_word'])\n",
    "\n",
    "        # As a 2d tensor, we need all rows to have the same length. So, we add -1 to the end of each list.\n",
    "        inputs['cue_indices'] = cue_indices + (10 - len(cue_indices)) * [-1]\n",
    "\n",
    "        all_features.append(inputs)\n",
    "    return all_features[0] if len(all_features) == 1 else all_features\n",
    "\n",
    "PREPROCESS_FUNC = {\n",
    "    'ana': blimp_to_features,\n",
    "    'dna': blimp_to_features,\n",
    "    'dnaa': blimp_to_features,\n",
    "    'rpsv': blimp_to_features,\n",
    "    'darn': blimp_to_features,\n",
    "    'NA': blimp_to_features,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "110a3d60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at BLIMP Dataset/roberta/train/cache-3c138be14de5eff0.arrow\n"
     ]
    }
   ],
   "source": [
    "SELECTED_GPU = 0\n",
    "MODEL_NAME = 'roberta'\n",
    "FIXED = False\n",
    "TASK = \"NA\"\n",
    "MAX_LENGTH = 32\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "PER_DEVICE_BATCH_SIZE = 64\n",
    "\n",
    "INPUT_MASKING = True\n",
    "MLM = True\n",
    "LEARNING_RATE = 3e-5\n",
    "LR_SCHEDULER_TYPE = \"linear\" \n",
    "WARMUP_RATIO = 0.1\n",
    "SEED = 42\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    load_from_disk,\n",
    "    load_metric,\n",
    ")\n",
    "\n",
    "from transformers import BertForMaskedLM, RobertaForMaskedLM\n",
    "# from customized_modeling_bert import BertForMaskedLM\n",
    "# from modeling.customized_modeling_roberta import RobertaForMaskedLM\n",
    "# from modeling.customized_modeling_electra import ElectraForMaskedLM\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "set_seed(SEED)\n",
    "\n",
    "# Load Dataset\n",
    "if TASK in BLIMP_TASKS:\n",
    "    data_path = f\"./BLIMP Dataset/{MODEL_NAME}/\"\n",
    "    data = load_from_disk(data_path)\n",
    "    train_data = data['train']\n",
    "    eval_data = data['test']\n",
    "else:\n",
    "    print(\"Not implemented yet!\")\n",
    "    exit()\n",
    "train_data = train_data.shuffle(SEED)\n",
    "num_labels = NUM_LABELS[TASK]\n",
    "\n",
    "# Download Tokenizer & Model\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH[MODEL_NAME], num_labels=num_labels)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH[MODEL_NAME])  \n",
    "\n",
    "if MODEL_NAME == \"bert\":\n",
    "    model = BertForMaskedLM.from_pretrained(MODEL_PATH[MODEL_NAME], config=config)\n",
    "elif MODEL_NAME == \"roberta\":\n",
    "    model = RobertaForMaskedLM.from_pretrained(MODEL_PATH[MODEL_NAME], config=config)\n",
    "# elif MODEL_NAME == \"electra\":\n",
    "#     model = ElectraForMaskedLM.from_pretrained(MODEL_PATH[MODEL_NAME], config=config)\n",
    "else:\n",
    "    print(\"model doesn't exist\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Preprocessing\n",
    "train_dataset = PREPROCESS_FUNC[TASK](train_data, tokenizer, MAX_LENGTH, input_masking=INPUT_MASKING, mlm=MLM)\n",
    "eval_dataset = PREPROCESS_FUNC[TASK](eval_data, tokenizer, MAX_LENGTH, input_masking=INPUT_MASKING, mlm=MLM)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn= default_data_collator, batch_size=PER_DEVICE_BATCH_SIZE)\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn= default_data_collator, batch_size=PER_DEVICE_BATCH_SIZE)\n",
    "\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "max_train_steps = NUM_TRAIN_EPOCHS * num_update_steps_per_epoch \n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = get_scheduler(\n",
    "        name=LR_SCHEDULER_TYPE,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=WARMUP_RATIO * max_train_steps,\n",
    "        num_training_steps=max_train_steps,\n",
    "    )\n",
    "\n",
    "# metric & Loss\n",
    "metric = load_metric(\"accuracy\")\n",
    "loss_fct = CrossEntropyLoss()\n",
    "\n",
    "tag = \"forseqclassification_\"\n",
    "tag += \"pretrained\" if FIXED else \"finetuned\" \n",
    "if MLM:\n",
    "    tag += \"_MLM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c2b39",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4aef8eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2763fb017034d7aa5bc77633727d9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'accuracy': 0.9904891304347826}\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "progress_bar = tqdm(range(max_train_steps))\n",
    "completed_steps = 0\n",
    "for epoch in range(NUM_TRAIN_EPOCHS):\n",
    "#     Train\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        good_token_id = batch.pop('good_token_id').to(device)\n",
    "        bad_token_id = batch.pop('bad_token_id').to(device)\n",
    "        cue_indices = batch.pop('cue_indices').to(device)\n",
    "        target_index = batch.pop('target_index').to(device) # size: [64]\n",
    "        batch.pop('labels').to(device)\n",
    "        batch = {k: v.to(device) for k, v in batch.items()} \n",
    "    \n",
    "        outputs = model(**batch) # outputs.logit.size(): [64, 32, 30522]\n",
    "        # To output the logits correspondig to target index, value zeroing's code changed the BertForMaskedLM in which\n",
    "        # the model outputs the logit directly. Now we use the original BertForMaskedLM and from it's output find the logits\n",
    "        # corresponding to target index as shown down below.\n",
    "        logits = outputs.logits[torch.arange(outputs.logits.size(0)), target_index] # size: [64, 30522]\n",
    "        good_logits = logits[torch.arange(logits.size(0)), good_token_id] # size: [64]\n",
    "        bad_logits = logits[torch.arange(logits.size(0)), bad_token_id] # size: [64]\n",
    "        \n",
    "        logits_of_interest = torch.stack([good_logits, bad_logits], dim=1) # size: [64, 2]\n",
    "        labels = torch.zeros(logits_of_interest.shape[0], dtype=torch.int64, device=device) # size: [64]        \n",
    "        loss = loss_fct(logits_of_interest, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        completed_steps += 1\n",
    "\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        if MLM:\n",
    "            good_token_id = batch.pop('good_token_id').to(device)\n",
    "            bad_token_id = batch.pop('bad_token_id').to(device)\n",
    "        target_index = batch.pop('target_index').to(device) # size: [64]\n",
    "        batch.pop('labels').to(device)\n",
    "        batch.pop('cue_indices').to(device)\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        logits = outputs.logits[torch.arange(outputs.logits.size(0)), target_index] # size: [64, 30522]\n",
    "\n",
    "        if MLM:\n",
    "            good_logits = logits[torch.arange(logits.size(0)), good_token_id]\n",
    "            bad_logits = logits[torch.arange(logits.size(0)), bad_token_id]\n",
    "            logits_of_interest = torch.stack([good_logits, bad_logits], dim=1)\n",
    "            labels = torch.zeros(logits_of_interest.shape[0], dtype=torch.int64, device=device)\n",
    "            predictions = torch.argmax(logits_of_interest, dim=-1)\n",
    "            metric.add_batch(predictions=predictions, references=labels)\n",
    "        else:\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            metric.add_batch(predictions=predictions, references=batch['labels'])\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "    print(f\"epoch {epoch}: {eval_metric}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24d54d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), f'{MODEL_NAME}_full_{tag}_epoch{NUM_TRAIN_EPOCHS}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436975cf",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b596113b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'{MODEL_NAME}_full_{tag}_epoch{NUM_TRAIN_EPOCHS}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1236948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: {'accuracy': 0.9904891304347826}\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    if MLM:\n",
    "        good_token_id = batch.pop('good_token_id').to(device)\n",
    "        bad_token_id = batch.pop('bad_token_id').to(device)\n",
    "    target_index = batch.pop('target_index').to(device) # size: [64]\n",
    "    batch.pop('labels').to(device)\n",
    "    batch.pop('cue_indices').to(device)\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits[torch.arange(outputs.logits.size(0)), target_index] # size: [64, 30522]\n",
    "\n",
    "    if MLM:\n",
    "        good_logits = logits[torch.arange(logits.size(0)), good_token_id]\n",
    "        bad_logits = logits[torch.arange(logits.size(0)), bad_token_id]\n",
    "        logits_of_interest = torch.stack([good_logits, bad_logits], dim=1)\n",
    "        labels = torch.zeros(logits_of_interest.shape[0], dtype=torch.int64, device=device)\n",
    "        predictions = torch.argmax(logits_of_interest, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=labels)\n",
    "    else:\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch['labels'])\n",
    "\n",
    "eval_metric = metric.compute()\n",
    "print(f\"epoch: {eval_metric}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d31cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sina_pt_gpu",
   "language": "python",
   "name": "sina_pt_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
